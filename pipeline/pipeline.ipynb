{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This pipeline is meant to be run on a kubernetes cluster only, Do not run on your local machine directly to avoid causing complications in your development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "!pip3 install pandas==0.24.2 matplotlib==3.2.2 scipy==1.4.1 statsmodels==0.12.0 scikit-learn==0.23.1 tensorflow==2.1.0 keras==2.3.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install or Upgrade if present the KFP library\n",
    "\n",
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the install was successful\n",
    "\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "\n",
    "# where the outputs are stored\n",
    "data_path = \"pipe_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Injestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_injestion(data_path):\n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "\n",
    "    import pandas as pd\n",
    "    \n",
    "    data = pd.read_csv('https://raw.githubusercontent.com/Chizzy-codes/g02-sms-spam/master/data/spam.csv', usecols=['v1', 'v2'],  encoding = 'latin-1')\n",
    "    data2 = pd.read_csv('https://raw.githubusercontent.com/Chizzy-codes/g02-sms-spam/master/data/spam_additional.csv', usecols=['Text', 'v3'],  encoding = 'latin-1')\n",
    "  \n",
    "    #Save the injested data as a pickle file to be used by the data tranformation component.\n",
    "    with open(f'{data_path}/inj_data', 'wb') as f:\n",
    "        pickle.dump((data, data2), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = data_injestion(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformation(data_path):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'spacy'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras'])\n",
    "    \n",
    "    from math import sqrt\n",
    "    from numpy import concatenate\n",
    "    from pandas import read_csv, DataFrame, concat\n",
    "    from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras import regularizers\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    embedding_dim = 100\n",
    "    max_length = 150\n",
    "    trunc_type='post'\n",
    "    padding_type='post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    \n",
    "    # Load and unpack the test_data\n",
    "    \n",
    "    with open(f'{data_path}/inj_data','rb') as f:\n",
    "        data, data2 = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n",
    "    spacy_stop_words = spacy.lang.en.STOP_WORDS # getting spacy's stop-words\n",
    "    \n",
    "    #Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\n",
    "    # Convert it to a Python list and paste it here\n",
    "    stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "    \n",
    "    stop_words = list(set(list(spacy_stop_words) + stopwords))\n",
    "    stopwords = list({word.lemma_.lower() for word in nlp(' '.join(stop_words))})\n",
    "    \n",
    "    # defining tokenzer function to tokenize the lower case lemma of documents in a corpus and \n",
    "    # filter out stop-words  \n",
    "    def tokenizer_spacy(text):\n",
    "        return [word.lemma_.lower() for word in nlp(text) if word.is_alpha and word.lemma_.lower() not in stopwords]\n",
    "    \n",
    "    #Replace the target colunms with binary number\n",
    "    data.replace({'ham':1,'spam':0},inplace=True)\n",
    "    \n",
    "    sentences = data['message']\n",
    "    labels=data['label']\n",
    "    \n",
    "    #Replace the target colunms with binary number\n",
    "    data.replace({'ham':1,'spam':0},inplace=True)\n",
    "    \n",
    "    senten = [word for word in sentences if word not in stopwords] # stopword filtering\n",
    "    \n",
    "    # tokenize (lemmatize and filter stop words) corpus \n",
    "    senten = [' '.join(tokenizer_spacy(doc)) for doc in senten]\n",
    "    \n",
    "    # word tokenizing\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(senten)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    vocab_size=len(word_index)\n",
    "\n",
    "    # padding and converting to numeric sequence\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    \n",
    "    \n",
    "    padded = np.array(padded)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    #Save the preprocessed data as a pickle file to be used by the model component.\n",
    "    with open(f'{data_path}/preprocessed_data', 'wb') as f:\n",
    "        pickle.dump((padded,labels), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data_transformation(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'keras==2.3.1'])\n",
    "    \n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "    \n",
    "    from keras.models import load_model\n",
    "    \n",
    "    model = load_model('model2.h5')\n",
    "    \n",
    "     # Load and unpack the test_data\n",
    "    \n",
    "    with open(f'{data_path}/preprocessed_data','rb') as f:\n",
    "        padded, labels = pickle.load(f)\n",
    "    \n",
    "    prediction = model.predict(padded)\n",
    "       \n",
    "    prediction = [int(np.round(i)) for i in prediction]\n",
    "           \n",
    "    with open(f'{data_path}/result', 'wb') as f:\n",
    "        pickle.dump(prediction, f)\n",
    "    \n",
    "    print('Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(data_path):\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    import pandas as pd\n",
    "    from math import sqrt\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from keras.models import load_model\n",
    "    \n",
    "    with open(f'{data_path}/preprocessed_data','rb') as f:\n",
    "        padded, labels = pickle.load(f)\n",
    "    \n",
    "    \n",
    "    with open(f'{data_path}/result', 'rb') as f:\n",
    "        prediction = pickle.load(f)\n",
    "    \n",
    "    f1 = 'F1 score: {:.4f}'.format(f1_score(labels, prediction))\n",
    "\n",
    "    precision = 'Precision_score: {:.4f}'.format(precision_score(labels, prediction))\n",
    "    \n",
    "    evaluation = [f1, precision]\n",
    "    \n",
    "    with open(f'{data_path}/result.txt', 'w') as f:\n",
    "        f.write(\" Evaluation: {}\".format(evaluation))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\" Predictions: {}\".format(prediction))\n",
    "        \n",
    "        \n",
    "    print('Done!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get = result(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create components.\n",
    "inj_op = comp.func_to_container_op(data_injestion , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "transformation_op = comp.func_to_container_op(data_transformation, base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = comp.func_to_container_op(predict, base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "result_op = comp.func_to_container_op(result, base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a client to enable communication with the Pipelines API server.\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "    name='Sms Spam Classification Pipeline',\n",
    "    description=\n",
    "    'A machine learning pipeline that makes predictions on whether on not the sms content of a csv file is spam or not.'\n",
    ")\n",
    "# Define parameters to be fed into pipeline\n",
    "def spam_pipeline(data_path: str):\n",
    "\n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(name=\"create_volume\",\n",
    "                       resource_name=\"data-volume\",\n",
    "                       size=\"1Gi\",\n",
    "                       modes=dsl.VOLUME_MODE_RWO)\n",
    "\n",
    "    # Create data injestion component.\n",
    "    injestion_container = inj_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create data transformation component.\n",
    "    transformation_container = transformation_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: injestion_container.pvolume})\n",
    "    # Create model training component.\n",
    "    predict_container = predict_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: transformation_container.pvolume})\n",
    "    \n",
    "    # Create model validation component.\n",
    "    result_container = result_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: predict_container.pvolume})\n",
    "    \n",
    "\n",
    "    # Print the result of the prediction\n",
    "    validation_result_container = dsl.ContainerOp(\n",
    "        name=\"print_validation_result\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: result_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/result.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = data_path\n",
    "\n",
    "pipeline_func = spam_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'spam_classification_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
